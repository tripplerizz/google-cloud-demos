{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fd85621-fd4c-4633-84f6-e0a6b634ae10",
   "metadata": {},
   "source": [
    "# Connecting to BigQuery\n",
    "## resource with problem statement of this demo\n",
    "- https://youtu.be/ieaqfU1BwJ8\n",
    "## create queries to filter data, extract features, and clean where possible\n",
    "\n",
    "### below is an **example** query\n",
    "- here a query is made and stored in dataframe\n",
    "- more tags, and api documentation are provided here: https://googleapis.dev/python/bigquery/latest/magics.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a97fa-c614-4c82-bb61-270e94b74588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86cd702-d275-4486-8359-7250c373119d",
   "metadata": {},
   "source": [
    "### example query to strip **html** , and replace \"|\" with \",\" for delimiting"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1412646-78b9-4179-bda6-f87591043bfc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "%%bigquery dataframe --verbose\n",
    "SELECT \n",
    "LOWER(CONCAT(title, \" \", REGEXP_REPLACE(body, \"<[^>]*>\", \"\"))) as article,\n",
    "REPLACE(tags, \"|\", \",\") AS  tags\n",
    "FROM `bigquery-public-data.stackoverflow.posts_questions`\n",
    "WHERE REGEXP_CONTAINS(tags, r\"(?:python|javascript|mobil|react|pandas)\")\n",
    "limit 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9815bd98-ae7d-40f1-9bbe-f2529496d0e8",
   "metadata": {},
   "source": [
    "### save query to python pandas dataframe\n",
    "- more data manipulation and rangling can be done in python if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc8f15-50bb-4a7b-93a2-c2abe1f7c95e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae64fa57-d518-4180-b51b-212897151c89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f4bc27-ddd7-42cf-919b-75a109a86f23",
   "metadata": {},
   "source": [
    "### python string formating -- **data preperation for model training**\n",
    "- code below removes all \",\" and new lines in article\n",
    "- then the tags are filtered and delimited by \",\"\n",
    "- this is done because training **csv** file is delimited by \",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a911a-d1aa-49ad-b758-b10c8ea52954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creating deep copy to test with\n",
    "test_dataframe = dataframe.copy(deep=True)\n",
    "\n",
    "# items selected for training\n",
    "interest_items = [\"python\", \"reactjs\", \"pandas\", \"numpy\", \"javascript\",\"sql\", \"jquery\", \"django\", \"php\"]\n",
    "with open(\"file_write.csv\", \"w\") as file_write:\n",
    "    for index, row in test_dataframe.iterrows():\n",
    "\n",
    "        #csv format requires delimiting by comma, appending comma at end of question\n",
    "        test_dataframe.at[index, 'article'] = test_dataframe.at[index, 'article'].replace(',','')\n",
    "        test_dataframe.at[index, 'article'] = test_dataframe.at[index, 'article'].replace('\\n','')\n",
    "        test_dataframe.at[index, 'article'] = test_dataframe.at[index, 'article'] + ','\n",
    "\n",
    "        clean_list = \"\"\n",
    "        #create iterable list of tags\n",
    "        row['tags'] = row['tags'].split(',') \n",
    "        for tag in row['tags']:\n",
    "            if tag in interest_items:\n",
    "                clean_list += tag + \",\" \n",
    "        # # delete rows which don't contain interest items\n",
    "        if not clean_list:\n",
    "            test_dataframe.drop(index, inplace=True)\n",
    "\n",
    "        else:\n",
    "            clean_list = clean_list[:-1]\n",
    "            test_dataframe.at[index, 'tags'] = clean_list\n",
    "            # writing datafram with filtered valuews into comma delimited csv\n",
    "            file_write.write(test_dataframe.at[index, 'article'] + clean_list + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1756ecf0-77f4-45ba-abdc-59ad4da82046",
   "metadata": {},
   "source": [
    "### note the difference between original and filtered dataframe\n",
    "- test_dataframe == filtered\n",
    "- dataframe == original\n",
    "- test_dataframe even has dropped certain rows, due to a lack of matched interest items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc8c775-b789-4104-8699-db2e63eecdd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa49ff-4ad3-4268-8da8-3a0987aa2030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b65112d-c108-4532-8295-36b98bf0a4cf",
   "metadata": {},
   "source": [
    "# After filtering and writing dataframe to CSV, upload to **Google CLoud Storage** (GCS)\n",
    "## the second cell runs a shell command. \n",
    "- use the \"!\" charector to initiate shell command from **jupitor notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996722a6-c6dc-46f4-b499-b31425e672f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil cp file_write.csv gs://[BUCKET-NAME]/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb7247d-1836-4c56-9c83-239bbb4662f4",
   "metadata": {},
   "source": [
    "# Creating a model, training and deploying\n",
    "## created based off of this resourece:https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/automl/automl-text-classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d6116-cf51-4735-b582-9a93e9cf372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install required libraries\n",
    "! pip install {USER_FLAG} --upgrade google-cloud-aiplatform google-cloud-storage jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b75848a-d349-4e27-aa9b-fa0cf36687db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform, storage, bigquery\n",
    "from google.protobuf import json_format\n",
    "\n",
    "REGION = \"[REGION]\"\n",
    "PROJECT_ID = \"[PROJECT_ID]\"\n",
    "BUCKET_NAME = \"[BUCKET_NAME]\"\n",
    "FILE_NAME = \"[FILE_NAME]\"\n",
    "\n",
    "src_uris =\"gs://{}/{}\".format(BUCKET_NAME, FILE_NAME)\n",
    "display_name = \"[DISPLAY_NAME -- your choice of naming\"\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64756d13-3e6f-48a7-8cf5-de7410456f94",
   "metadata": {},
   "source": [
    "## create text multi label dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480cca9b-5aa2-4db2-b8f1-6b96e849677f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creating text dataset\n",
    "ds = aiplatform.TextDataset.create(\n",
    "    display_name=display_name,\n",
    "    gcs_source=src_uris,\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.text.multi_label_classification,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce68c2-2863-440a-8f69-70845cb42ee6",
   "metadata": {},
   "source": [
    "## pull dataset id of most recent dataset created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a564fb6-bd40-494f-82e4-69e47a214b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = ds.resource_name.split(\"/\")[-1]\n",
    "text_dataset = aiplatform.TextDataset(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d1332-d558-4949-a970-398176b244c3",
   "metadata": {},
   "source": [
    "## create training job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0a0c92-fc01-4f4d-83f1-39ca0ced04ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training job\n",
    "training_job_display_name = \"stack_question_training\" # change to your liking\n",
    "job = aiplatform.AutoMLTextTrainingJob(\n",
    "    display_name=training_job_display_name,\n",
    "    prediction_type=\"classification\",\n",
    "    multi_label=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed6478-12fe-4e33-a23f-faa067d252ae",
   "metadata": {},
   "source": [
    "## run model training\n",
    "- this took about 4 hours in this test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646b544-1def-4ea0-9cb9-99fed4ecd547",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_display_name = \"stack_question_model\" # change to your liking\n",
    "\n",
    "# Run the training job\n",
    "model = job.run(\n",
    "    dataset=text_dataset,\n",
    "    model_display_name=model_display_name,\n",
    "    training_fraction_split=0.7,\n",
    "    validation_fraction_split=0.2,\n",
    "    test_fraction_split=0.1,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707df095-4967-4ffa-b388-a44b1b55b94a",
   "metadata": {},
   "source": [
    "##  deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458e1c8-b257-42b6-95c6-aef1198105f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model_display_name = \"model-endpoint-stack-questions\" # change to your liking\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=deployed_model_display_name, sync=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e2867c-7a51-48ee-8fb7-69101e27310b",
   "metadata": {},
   "source": [
    "# once model is deployed: \n",
    "- find model endpoint id through vertex ai \"endpoints\" page on **Google Cloud Console**\n",
    "- example: \"[GOOGLE_CLOUD_CONSOLE]/vertex-ai/endpoints?project=[YOUR PROJECT HERE]\"\n",
    "- funciton below is pulled from: https://github.com/googleapis/python-aiplatform/blob/main/samples/snippets/prediction_service/predict_text_classification_single_label_sample.py\n",
    "- **this function** will test your deployed model and is an example of how to use your model to recieve predictions\n",
    "\n",
    "## visualize performance of your model\n",
    "- navigate to \"model\" page of your vertex ai dashboard\n",
    "- here you can inspect the performance of each of your models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f91f4-989f-4ffc-ba71-c24265511d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text_classification_multi_lable(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    content: str,\n",
    "    location: str ,\n",
    "    api_endpoint: str = \"[your model location]-aiplatform.googleapis.com\",\n",
    "):\n",
    "    api_endpoint = location + \"-aiplatform.googleapis.com\"\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    instance = predict.instance.TextClassificationPredictionInstance(\n",
    "        content=content,\n",
    "    ).to_value()\n",
    "    instances = [instance]\n",
    "    parameters_dict = {}\n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    print(\"response\")\n",
    "    print(\" deployed_model_id:\", response.deployed_model_id)\n",
    "    # See gs://google-cloud-aiplatform/schema/predict/prediction/text_classification.yaml for the format of the predictions.\n",
    "    predictions = response.predictions\n",
    "    for prediction in predictions:\n",
    "        dictionary = dict(prediction)\n",
    "        print(\" prediction:\")\n",
    "        for index in range(len(dictionary['displayNames'])):\n",
    "            print(\"\\t\", dictionary['displayNames'][index], \": \" , dictionary['confidences'][index])\n",
    "    return dictionary \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b72e97-4aaa-4aef-a0a1-993cde463026",
   "metadata": {},
   "outputs": [],
   "source": [
    "results =  predict_text_classification_multi_lable(\n",
    "            \"[PROJECT_ID]\",\n",
    "            endpoint_id=\"[YOUR ENDPOINT ID]\",\n",
    "            location=\"[LOCATION FOR YOUR ENDPOINT]\",\n",
    "            content= \"CONTENT YOU WANT TO TEST\"\n",
    "        ) "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
